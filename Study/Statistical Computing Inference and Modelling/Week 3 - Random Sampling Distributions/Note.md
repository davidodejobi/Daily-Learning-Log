## ðŸ§  Overview

**Weekâ€¯3** introduces the logic of **random sampling** and explains how sample statisticsâ€”such as the sample meanâ€”follow their probability distributions. These ideas underpin all inferential statistics because they tell us *how much* a sample can be expected to vary from one draw to the next.

---

## ðŸŽ¯ Learning Objectives

By the end of this lesson, you should be able to:

1. **Define** random sampling and explain why it matters.
2. **Distinguish** sampling *with* vs. *without* replacement.
3. **Apply and compare** four probabilityâ€‘based sampling designs (simple, systematic, stratified, cluster).
4. **Explain** the concept of a *sampling distribution* and compute a **standard error**.
5. **Use** the **Central Limit Theorem (CLT)** to approximate probabilities for sample means.

---

## 1. Why Random Sampling?

Random sampling gives every population member a *known, nonâ€‘zero* chance of selection. This prevents hidden biases and lets us quantify sampling uncertainty with probability theory.

### 1.1 Sampling Frame

Before drawing a sample, assemble a **sampling frame**â€”a complete list of population units (e.g., a roster of students).

### 1.2 With vs. Without Replacement

| Style                   | Independence?                             | Same unit can appear twice? | Example                                                                       |
| ----------------------- | ----------------------------------------- | --------------------------- | ----------------------------------------------------------------------------- |
| **With Replacement**    | Yes                                       | Yes                         | Drawing numbered balls from a bag and putting each back before the next draw. |
| **Without Replacement** | No (probabilities change after each draw) | No                          | Lottery where each ticket is removed once it wins.                            |

---

## 2. Core Probability Sampling Designs

### 2.1 Simple Random Sampling (SRS)

* **How:** Every combination of *n* units is equally likely.
* **Selection probability:** *nâ€¯/â€¯N* for each unit.
* **When useful:** Homogeneous populations, small to moderate N.

### 2.2 Systematic Sampling

* **How:** Choose a random start between 1 and *k*, then take every *k*â€‘th unit (where *kâ€¯=â€¯Nâ€¯/â€¯n*).
* **Pros:** Easy to implement in the field.
* **Caution:** Avoid if the population list has hidden periodicity.

### 2.3 Stratified Sampling

* **How:** Divide the population into **strata** that are internally similar. Draw an SRS within each stratum (proportionally or with oversampling).
* **Benefit:** Improves precision when strata differ strongly (e.g., age groups in a health survey).

### 2.4 Cluster Sampling

* **How:** Break the population into naturally occurring **clusters** (e.g., classrooms). Randomly select clusters, then survey *all* units within each selected cluster.
* **Benefit:** Reduces travel/field costs.
* **Tradeâ€‘off:** Larger sampling error unless clusters are internally diverse.

---

## 3. Sampling Distribution of the Sample Mean

Given a population with mean **Î¼** and standard deviation **Ïƒ**:

* The **sample mean** $\bar{X}$ from an SRS of size *n* is itself a random variable.
* **Expected value:** $E[\bar{X}] = Î¼$
  ($\bar{X}$ is an *unbiased* estimator.)
* **Standard error (SE):**  $SE(\bar{X}) = \dfrac{Ïƒ}{\sqrt{n}}$
  (smaller SE â†’ more precise estimates).

> **Finiteâ€‘population correction:** If sampling without replacement and the sample is a large fraction of N, multiply SE by $\sqrt{1 âˆ’ n/N}$.

---

## 4. Central Limit Theorem (CLT)

Regardless of the populationâ€™s shape, the distribution of $\bar{X}$ approaches a **Normal** distribution as *n* becomes large (â‰ˆÂ 30+ for mild skew, larger for heavy skew).

### 4.1 Practical Rule

* Convert $\bar{X}$ to a *Z*â€‘score:
  $Z = \frac{\bar{X} âˆ’ Î¼}{SE(\bar{X})}$
* Use the standard Normal table to find probabilities or critical values.

### 4.2 Quick Example

Population: Î¼Â =Â 60, ÏƒÂ =Â 10, sample size nÂ =Â 36.
Probability that the sample mean is **below 62**:

1. SEÂ =Â 10â€¯/â€¯âˆš36Â =Â 1.667.
2. ZÂ =Â (62Â âˆ’Â 60)â€¯/â€¯1.667Â â‰ˆÂ 1.20.
3. P(ZÂ <Â 1.20)Â â‰ˆÂ 0.884 (88.4â€¯%).

---

## 5. Worked Practice Problems

1. **SRS Probability**
   NÂ =Â 1â€¯000, nÂ =Â 100.
   a) P(unit selected)Â =Â 0.10.
   b) P(unit *not* selected)Â =Â 0.90.
2. **Systematic Sampling Indexing**
   NÂ =Â 300, nÂ =Â 20 â‡’ kÂ =Â 15.
   Random startÂ =Â 7 â‡’ sample indicesÂ =Â 7,â€¯22,â€¯37,â€¯â€¦
3. **Standard Error & CLT**
   Î¼Â =Â 50, ÏƒÂ =Â 8, nÂ =Â 64Â â†’ SEÂ =Â 1.
   P($\bar{X}Â >Â 52$) â‡’ ZÂ =Â 2 â‡’ PÂ â‰ˆÂ 0.0228.
4. **Withoutâ€‘Replacement Draw**
   Bag: 5Â red,Â 3Â blue,Â 2Â green.
   P(blue on second draw | no replacement)Â =Â (3/10)Â·(7/9)Â + (7/10)Â·(3/9)Â =Â 0.233.

---

## 6. Reflection Questions

* **Design Choice:** When would *cluster* sampling be preferable even if it increases variance?
* **Precision:** How does doubling n affect SE? *(Hint: SE halves.)*
* **CLT Limits:** What population features (e.g., heavy tails) slow the convergence to Normality?

---

**Next Steps:** Use a statistical package (R/Python) to simulate thousands of SRS draws from a nonâ€‘Normal population. Plot the resulting distribution of $\bar{X}$ and watch the bell curve emerge!
